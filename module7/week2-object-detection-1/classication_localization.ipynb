{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtVJpR3eQQ5_"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "data_dir = kagglehub.dataset_download(\"andrewmvd/dog-and-cat-detection\")\n",
        "print(data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import seaborn as sns\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision.models.resnet import ResNet18_Weights"
      ],
      "metadata": {
        "id": "dvvckRQTRRJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, annotations_dir, image_dir, transform=None):\n",
        "        self.annotations_dir = annotations_dir\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = self.filter_images_with_single_object()\n",
        "\n",
        "    def filter_images_with_single_object(self):\n",
        "        valid_image_files = []\n",
        "        for f in os.listdir(self.image_dir):\n",
        "            if os.path.isfile(os.path.join(self.image_dir, f)):\n",
        "                img_name = f\n",
        "                annotation_name = os.path.splitext(img_name)[0] + \".xml\"\n",
        "                annotation_path = os.path.join(self.annotations_dir, annotation_name)\n",
        "                if self.count_objects_in_annotation(annotation_path) == 1:\n",
        "                    valid_image_files.append(img_name)\n",
        "        return valid_image_files\n",
        "\n",
        "    def count_objects_in_annotation(self, annotation_path):\n",
        "        try:\n",
        "            tree = ET.parse(annotation_path)\n",
        "            root = tree.getroot()\n",
        "            count = 0\n",
        "            for obj in root.findall(\"object\"):\n",
        "                count += 1\n",
        "            return count\n",
        "        except FileNotFoundError:\n",
        "            return 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Image path\n",
        "        img_name = self.image_files[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Annotation path\n",
        "        annotation_name = os.path.splitext(img_name)[0] + \".xml\"\n",
        "        annotation_path = os.path.join(self.annotations_dir, annotation_name)\n",
        "\n",
        "        # Parse annotation to get label and bounding box\n",
        "        label, bbox = self.parse_annotation(annotation_path)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label, bbox\n",
        "\n",
        "    def parse_annotation(self, annotation_path):\n",
        "        try:\n",
        "            tree = ET.parse(annotation_path)\n",
        "            root = tree.getroot()\n",
        "\n",
        "            # Get image size for normalization\n",
        "            image_width = int(root.find(\"size/width\").text)\n",
        "            image_height = int(root.find(\"size/height\").text)\n",
        "\n",
        "            label = None\n",
        "            bbox = None\n",
        "            for obj in root.findall(\"object\"):\n",
        "                name = obj.find(\"name\").text\n",
        "                if label is None:  # Take the first label\n",
        "                    label = name\n",
        "\n",
        "                # Get bounding box coordinates\n",
        "                xmin = int(obj.find(\"bndbox/xmin\").text)\n",
        "                ymin = int(obj.find(\"bndbox/ymin\").text)\n",
        "                xmax = int(obj.find(\"bndbox/xmax\").text)\n",
        "                ymax = int(obj.find(\"bndbox/ymax\").text)\n",
        "\n",
        "                # Normalize bbox coordinates to [0, 1]\n",
        "                bbox = [\n",
        "                    xmin / image_width,\n",
        "                    ymin / image_height,\n",
        "                    xmax / image_width,\n",
        "                    ymax / image_height,\n",
        "                ]\n",
        "\n",
        "            # Convert label to numerical representation (0 for cat, 1 for dog)\n",
        "            label_num = 0 if label == \"cat\" else 1 if label == \"dog\" else -1\n",
        "\n",
        "            return label_num, torch.tensor(bbox, dtype=torch.float32)\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing annotation: {e}\")\n",
        "            return -1, torch.zeros(4, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "YzB2zY46RXhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data directory\n",
        "annotations_dir = os.path.join(data_dir, 'annotations')\n",
        "image_dir = os.path.join(data_dir, 'images')\n",
        "\n",
        "# Get list of image files and create a dummy dataframe to split the data\n",
        "image_files = [f for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))]\n",
        "df = pd.DataFrame({'image_name': image_files})\n",
        "\n",
        "# Split data\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "DxWOSR0mSI07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Datasets\n",
        "train_dataset = ImageDataset(annotations_dir, image_dir, transform=transform)\n",
        "val_dataset = ImageDataset(annotations_dir, image_dir, transform=transform)\n",
        "\n",
        "# Filter datasets based on train_df and val_df\n",
        "train_dataset.image_files = [f for f in train_dataset.image_files if f in train_df['image_name'].values]\n",
        "val_dataset.image_files = [f for f in val_dataset.image_files if f in val_df['image_name'].values]\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "5NwMlWAdSSVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoHeadedModel(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(TwoHeadedModel, self).__init__()\n",
        "\n",
        "        # Base model\n",
        "        self.base_model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "        self.num_ftrs = self.base_model.fc.in_features\n",
        "\n",
        "        # Remove the original fully connected layer\n",
        "        self.base_model.fc = nn.Identity()\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Linear(self.num_ftrs, num_classes)\n",
        "\n",
        "        # Bounding box regression head\n",
        "        self.regressor = nn.Linear(self.num_ftrs, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward through the base model\n",
        "        x = self.base_model(x)\n",
        "\n",
        "        # Classification logits\n",
        "        class_logits = self.classifier(x)\n",
        "\n",
        "        # Bounding box coordinates (normalized)\n",
        "        bbox_coords = torch.sigmoid(self.regressor(x))\n",
        "\n",
        "        return class_logits, bbox_coords"
      ],
      "metadata": {
        "id": "xU2th-cPSWEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TwoHeadedModel()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "criterion_class = nn.CrossEntropyLoss() # For classification\n",
        "criterion_bbox = nn.MSELoss() # For bounding box regression\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "hUahhF8OSaxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Training Loop"
      ],
      "metadata": {
        "id": "ENy5VGwjSbPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    for batch_idx, (data, targets, bboxes) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        targets = targets.to(device)\n",
        "        bboxes = bboxes.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        scores, pred_bboxes = model(data)\n",
        "\n",
        "        # Compute losses\n",
        "        loss_class = criterion_class(scores, targets)  # Classification loss\n",
        "        loss_bbox = criterion_bbox(pred_bboxes, bboxes)  # Bounding box regression loss\n",
        "        loss = loss_class + loss_bbox  # Combine losses\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        total_loss_bbox = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        for data, targets, bboxes in val_loader:\n",
        "            data = data.to(device)\n",
        "            targets = targets.to(device)\n",
        "            bboxes = bboxes.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            scores, pred_bboxes = model(data)\n",
        "\n",
        "            # Compute classification accuracy\n",
        "            _, predictions = scores.max(1)\n",
        "            correct += (predictions == targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "\n",
        "            # Compute bounding box loss for monitoring\n",
        "            total_loss_bbox += criterion_bbox(pred_bboxes, bboxes).item() * data.size(0)\n",
        "            total_samples += data.size(0)\n",
        "\n",
        "        # Calculate metrics\n",
        "        validation_accuracy = (correct / total) * 100\n",
        "        avg_loss_bbox = total_loss_bbox / total_samples\n",
        "\n",
        "        # Print validation results\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Accuracy: {validation_accuracy:.2f}%, \"\n",
        "              f\"Avg. Bbox Loss: {avg_loss_bbox:.4f}\")"
      ],
      "metadata": {
        "id": "QE1B_dnxVGLd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}