{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3CA6mAhVZ8R"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "data_dir = kagglehub.dataset_download(\"andrewmvd/dog-and-cat-detection\")\n",
        "print(data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import seaborn as sns\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision.models.resnet import ResNet18_Weights, ResNet50_Weights"
      ],
      "metadata": {
        "id": "K9-6vM_RWIk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, annotations_dir, image_dir, transform=None):\n",
        "        self.annotations_dir = annotations_dir\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = self.filter_images_with_multiple_objects()\n",
        "\n",
        "    def filter_images_with_multiple_objects(self):\n",
        "        valid_image_files = []\n",
        "        for f in os.listdir(self.image_dir):\n",
        "            if os.path.isfile(os.path.join(self.image_dir, f)):\n",
        "                img_name = f\n",
        "                annotation_name = os.path.splitext(img_name)[0] + \".xml\"\n",
        "                annotation_path = os.path.join(self.annotations_dir, annotation_name)\n",
        "                if self.count_objects_in_annotation(annotation_path) == 1:\n",
        "                    valid_image_files.append(img_name)\n",
        "        return valid_image_files\n",
        "\n",
        "    def count_objects_in_annotation(self, annotation_path):\n",
        "        try:\n",
        "            tree = ET.parse(annotation_path)\n",
        "            root = tree.getroot()\n",
        "            count = 0\n",
        "            for obj in root.findall(\"object\"):\n",
        "                count += 1\n",
        "            return count\n",
        "        except FileNotFoundError:\n",
        "            return 0\n",
        "\n",
        "    def parse_annotation(self, annotation_path):\n",
        "        tree = ET.parse(annotation_path)\n",
        "        root = tree.getroot()\n",
        "        label = None\n",
        "        bbox = None\n",
        "\n",
        "        image_width = int(root.find(\"size/width\").text)\n",
        "        image_height = int(root.find(\"size/height\").text)\n",
        "\n",
        "        for obj in root.findall(\"object\"):\n",
        "            name = obj.find(\"name\").text\n",
        "            if label is None:  # Take the first label\n",
        "                label = name\n",
        "            # Get bounding box coordinates\n",
        "            xmin = int(obj.find(\"bndbox/xmin\").text)\n",
        "            ymin = int(obj.find(\"bndbox/ymin\").text)\n",
        "            xmax = int(obj.find(\"bndbox/xmax\").text)\n",
        "            ymax = int(obj.find(\"bndbox/ymax\").text)\n",
        "            # Normalize bbox coordinates to [0, 1]\n",
        "            bbox = [\n",
        "                xmin / image_width,\n",
        "                ymin / image_height,\n",
        "                xmax / image_width,\n",
        "                ymax / image_height,\n",
        "            ]\n",
        "        # Convert label to numerical representation (0 for cat, 1 for dog)\n",
        "        label_num = 0 if label == \"cat\" else 1 if label == \"dog\" else -1\n",
        "        return label_num, torch.tensor(bbox, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load first image\n",
        "        img1_file = self.image_files[idx]\n",
        "        img1_path = os.path.join(self.image_dir, img1_file)\n",
        "        annotation_name = os.path.splitext(img1_file)[0] + \".xml\"\n",
        "        img1_annotations = self.parse_annotation(\n",
        "            os.path.join(self.annotations_dir, annotation_name)\n",
        "        )\n",
        "\n",
        "        # Load second image (randomly selected)\n",
        "        idx2 = random.randint(0, len(self.image_files) - 1)\n",
        "        img2_file = self.image_files[idx2]\n",
        "        img2_path = os.path.join(self.image_dir, img2_file)\n",
        "        annotation_name = os.path.splitext(img2_file)[0] + \".xml\"\n",
        "        img2_annotations = self.parse_annotation(\n",
        "            os.path.join(self.annotations_dir, annotation_name)\n",
        "        )\n",
        "\n",
        "        # Open images\n",
        "        img1 = Image.open(img1_path).convert(\"RGB\")\n",
        "        img2 = Image.open(img2_path).convert(\"RGB\")\n",
        "\n",
        "        # Create a horizontally merged image\n",
        "        merged_w = img1.width + img2.width\n",
        "        merged_h = max(img1.height, img2.height)\n",
        "        merged_image = Image.new(\"RGB\", (merged_w, merged_h))\n",
        "        merged_image.paste(img1, (0, 0))\n",
        "        merged_image.paste(img2, (img1.width, 0))\n",
        "\n",
        "        # Adjust bounding boxes for merged image\n",
        "        merged_annotations = []\n",
        "        # First image annotations (unchanged)\n",
        "        merged_annotations.append(\n",
        "            {\"bbox\": img1_annotations[1].tolist(), \"label\": img1_annotations[0]}\n",
        "        )\n",
        "        # Second image annotations (adjust coordinates)\n",
        "        new_bbox = [\n",
        "            (img2_annotations[1][0] * img2.width + img1.width) / merged_w,\n",
        "            img2_annotations[1][1] * img2.height / merged_h,\n",
        "            (img2_annotations[1][2] * img2.width + img1.width) / merged_w,\n",
        "            img2_annotations[1][3] * img2.height / merged_h,\n",
        "        ]\n",
        "        merged_annotations.append({\"bbox\": new_bbox, \"label\": img2_annotations[0]})\n",
        "\n",
        "        # Transform merged image to tensor\n",
        "        if self.transform:\n",
        "            merged_image = self.transform(merged_image)\n",
        "        else:\n",
        "            merged_image = transforms.ToTensor()(merged_image)\n",
        "\n",
        "        # Convert annotations to tensors\n",
        "        annotations = torch.zeros((len(merged_annotations), 5))\n",
        "        for i, ann in enumerate(merged_annotations):\n",
        "            annotations[i] = torch.cat(\n",
        "                (torch.tensor(ann[\"bbox\"]), torch.tensor([ann[\"label\"]]))\n",
        "            )\n",
        "\n",
        "        return merged_image, annotations"
      ],
      "metadata": {
        "id": "pXhGaqzUXEcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "annotations_dir = os.path.join(data_dir, 'annotations')\n",
        "image_dir = os.path.join(data_dir, 'images')\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "dataset = MyDataset(annotations_dir, image_dir, transform=transform)\n",
        "train_dataset, val_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)"
      ],
      "metadata": {
        "id": "kp5UCkepZCyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleYOLO(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SimpleYOLO, self).__init__()\n",
        "        self.backbone = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Remove the final classification layer of ResNet\n",
        "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n",
        "\n",
        "        # YOLO head: Linear layer\n",
        "        # Output size: 2 grid cells, 4 bbox coords + num_classes per cell\n",
        "        self.fcs = nn.Linear(2048, 2 * 2 * (4 + self.num_classes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch, C, H, W)\n",
        "        features = self.backbone(x)\n",
        "        features = F.adaptive_avg_pool2d(features, (1, 1))  # (batch, 2048, 1, 1)\n",
        "        features = features.view(features.size(0), -1)  # (batch, 2048)\n",
        "        features = self.fcs(features)\n",
        "        return features"
      ],
      "metadata": {
        "id": "THtN3ThhanrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "num_classes = 2  # Assuming two classes: dog and cat\n",
        "class_to_idx = {'dog': 0, 'cat': 1}\n",
        "\n",
        "model = SimpleYOLO(num_classes=num_classes).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "iojXXjswa3Zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_loss(output, targets, device, num_classes):\n",
        "    mse_loss = nn.MSELoss()\n",
        "    ce_loss = nn.CrossEntropyLoss()\n",
        "    batch_size = output.shape[0]\n",
        "    total_loss = 0\n",
        "\n",
        "    # (batch, grid_y, grid_x, 4 (bboxs) + num_classes)\n",
        "    output = output.view(batch_size, 2, 2, 4 + num_classes)\n",
        "\n",
        "    for i in range(batch_size):  # Iterate through each image in the batch\n",
        "        for j in range(len(targets[i])):  # Iterate through objects in an image\n",
        "            # Determine the grid cell for the object\n",
        "            # Assuming bbox coordinates are normalized to [0, 1]\n",
        "            bbox_center_x = (targets[i][j][0] + targets[i][j][2]) / 2\n",
        "            bbox_center_y = (targets[i][j][1] + targets[i][j][3]) / 2\n",
        "\n",
        "            # Multiply by number of grid cells (2 in this case )\n",
        "            grid_x = int(bbox_center_x * 2)  # Grid cell X\n",
        "            grid_y = int(bbox_center_y * 2)  # Grid cell Y\n",
        "\n",
        "            # 1. Classification Loss for the responsible grid cell\n",
        "            # Convert label to one -hot encoding only for this example\n",
        "            # One hot encoding\n",
        "            label_one_hot = torch.zeros(num_classes, device=device)\n",
        "            label_one_hot[int(targets[i][j][4])] = 1\n",
        "\n",
        "            # Classification loss (using CrossEntropyLoss)\n",
        "            classification_loss = ce_loss(output[i, grid_y, grid_x, 4:], label_one_hot)\n",
        "\n",
        "            # Regression Loss\n",
        "            bbox_target = targets[i][j][:4].to(device)\n",
        "            regression_loss = mse_loss(output[i, grid_y, grid_x, :4], bbox_target)\n",
        "\n",
        "            # No Object Loss\n",
        "            no_obj_loss = 0\n",
        "            for other_grid_y in range(2):\n",
        "                for other_grid_x in range(2):\n",
        "                    if other_grid_y != grid_y or other_grid_x != grid_x:\n",
        "                        # MSE loss for predicting no object (all zeros )\n",
        "                        no_obj_loss += mse_loss(output[i, other_grid_y, other_grid_x, :4],\n",
        "                                                torch.zeros(4, device=device))\n",
        "\n",
        "            total_loss += classification_loss + regression_loss + no_obj_loss\n",
        "\n",
        "    return total_loss / batch_size"
      ],
      "metadata": {
        "id": "0BTZaW4UcBH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, data_loader, device, num_classes):\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "    running_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in tqdm.tqdm(data_loader, desc=\"Validation\", leave=False):\n",
        "            images = images.to(device)\n",
        "            output = model(images)\n",
        "            total_loss = calculate_loss(output, targets, device, num_classes)\n",
        "            running_loss += total_loss.item()\n",
        "\n",
        "            # Reshape output to (batch_size, grid_y, grid_x, 4 + num_classes)\n",
        "            output = output.view(images.shape[0], 2, 2, 4 + num_classes)\n",
        "\n",
        "            # Collect predictions and targets\n",
        "            for batch_idx in range(images.shape[0]):\n",
        "                for target in targets[batch_idx]:\n",
        "                    # Determine responsible grid cell\n",
        "                    bbox_center_x = (target[0] + target[2]) / 2\n",
        "                    bbox_center_y = (target[1] + target[3]) / 2\n",
        "                    grid_x = int(bbox_center_x * 2)\n",
        "                    grid_y = int(bbox_center_y * 2)\n",
        "\n",
        "                    # Class prediction\n",
        "                    prediction = output[batch_idx, grid_y, grid_x, 4:].argmax().item()\n",
        "                    all_predictions.append(prediction)\n",
        "                    all_targets.append(target[4].item())\n",
        "\n",
        "    val_loss = running_loss / len(data_loader)\n",
        "\n",
        "    # Convert lists to tensors for PyTorch ’s metric functions\n",
        "    all_predictions = torch.tensor(all_predictions, device=device)\n",
        "    all_targets = torch.tensor(all_targets, device=device)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    val_accuracy = (all_predictions == all_targets).float().mean()\n",
        "\n",
        "    return val_loss, val_accuracy.item()"
      ],
      "metadata": {
        "id": "JIqAke8hc5NL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, optimizer, num_epochs, device, num_classes):\n",
        "    best_val_accuracy = 0.0\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in tqdm.tqdm(range(num_epochs), desc=\"Epochs\"):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for images, targets in tqdm.tqdm(train_loader, desc=\"Batches\", leave=False):\n",
        "            images = images.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(images)\n",
        "            total_loss = calculate_loss(output, targets, device, num_classes)\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += total_loss.item()\n",
        "\n",
        "        # Calculate training loss\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        train_losses.append(epoch_loss)\n",
        "\n",
        "        # Validation phase\n",
        "        val_loss, val_accuracy = evaluate_model(model, val_loader, device, num_classes)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, \"\n",
        "            f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\"\n",
        "        )\n",
        "\n",
        "        # Save the best model\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            torch.save(model.state_dict(), \"best_model.pth\")\n",
        "\n",
        "    return train_losses, val_losses, val_accuracies"
      ],
      "metadata": {
        "id": "ylE1JUKRgrRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(model, image_path, transform, device, class_to_idx, threshold=0.5):\n",
        "    # Load and preprocess the image\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    original_width, original_height = image.size\n",
        "\n",
        "    # Resize the image to match the model's input size (e.g., 448x448)\n",
        "    resized_image = image.resize((448, 448))\n",
        "    resized_width, resized_height = resized_image.size\n",
        "\n",
        "    # Apply transformations\n",
        "    transformed_image = transform(resized_image).unsqueeze(0).to(device)\n",
        "\n",
        "    # Forward pass through the model\n",
        "    with torch.no_grad():\n",
        "        output = model(transformed_image)\n",
        "\n",
        "        # Reshape output for a 2x2 grid structure\n",
        "        output = output.view(1, 2, 2, 4 + len(class_to_idx))\n",
        "\n",
        "    # Visualization setup\n",
        "    fig, ax = plt.subplots(1)\n",
        "    ax.imshow(resized_image)\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "    # Process each grid cell\n",
        "    for grid_y in range(2):\n",
        "        for grid_x in range(2):\n",
        "            # Class prediction and bounding box\n",
        "            class_pred = output[0, grid_y, grid_x, 4:].argmax().item()\n",
        "            bbox = output[0, grid_y, grid_x, :4].tolist()\n",
        "\n",
        "            # Confidence ( probability of the predicted class )\n",
        "            confidence = torch.softmax(output[0, grid_y, grid_x, 4:], dim=0)[class_pred].item()\n",
        "\n",
        "            # Scale bounding box back to the image dimensions\n",
        "            x_min = bbox[0] * (resized_width / 2) + grid_x * (resized_width / 2)\n",
        "            y_min = bbox[1] * (resized_height / 2) + grid_y * (resized_height / 2)\n",
        "            x_max = bbox[2] * (resized_width / 2) + grid_x * (resized_width / 2)\n",
        "            y_max = bbox[3] * (resized_height / 2) + grid_y * (resized_height / 2)\n",
        "\n",
        "            # Draw bounding box and label if confidence exceeds the threshold\n",
        "            if confidence > threshold:\n",
        "                rect = patches.Rectangle(\n",
        "                    (x_min, y_min),\n",
        "                    x_max - x_min,\n",
        "                    y_max - y_min,\n",
        "                    linewidth=1,\n",
        "                    edgecolor=\"r\",\n",
        "                    facecolor=\"none\"\n",
        "                )\n",
        "                ax.add_patch(rect)\n",
        "                plt.text(\n",
        "                    x_min,\n",
        "                    y_min,\n",
        "                    f\"{list(class_to_idx.keys())[class_pred]}: {confidence:.2f}\",\n",
        "                    fontsize=12,\n",
        "                    bbox=dict(facecolor=\"red\", alpha=0.5),\n",
        "                    color=\"white\"\n",
        "                )\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "YacQJUzGg8N0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model\n",
        "model.load_state_dict(torch.load(\"best_model.pth\") )"
      ],
      "metadata": {
        "id": "i-TPIKNeiDBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10  # Number of training epochs\n",
        "train_losses, val_losses, val_accuracies = train_model(model, train_loader, val_loader, optimizer, num_epochs, device, num_classes)\n"
      ],
      "metadata": {
        "id": "mfsBBQckiBWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model's state_dict()\n",
        "torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "# Example of loading the model later\n",
        "model = SimpleYOLO(num_classes=num_classes).to(device) # Initialize the model architecture first\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "model.eval() # Set the model to evaluation mode"
      ],
      "metadata": {
        "id": "xJo_YszMiTkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference on a sample image\n",
        "image_path = \"/mnt/c/Study/OD Project/good_1.jpg\"\n",
        "inference(model, image_path, transform, device, class_to_idx, threshold=0.5)"
      ],
      "metadata": {
        "id": "6nR_ZrHciO_6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}